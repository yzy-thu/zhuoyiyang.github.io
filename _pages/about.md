---
permalink: /
title: "Zhuoyi Yang – Tsinghua University"
author_profile: true
redirect_from: 
  - /about/
  - /about.html

---

### About Me
I am a PhD Candidate at **Tsinghua University**, with research interests both in multimodal generation and understanding.
My work focuses on **text-to-image/video generation**, **diffusion models**, and **visual understanding**. I have contributed to projects including the CogVideo, CogView, and CogVLM series.
I am passionate about exploring generative AI and developing models that bridge vision and language for creative content generation.

**I am currently seeking visiting opportunities** to collaborate with researchers and expand my expertise in multimodal AI.

---

### Selected Publications


**Kaleido: Open-Sourced Multi-Subject Reference Video Generation Model**
<small>Zhenxing Zhang, Jiayan Teng, **Zhuoyi Yang**, Tiankun Cao, et al.</small>
*arXiv Preprint*
[arXiv](https://arxiv.org/abs/2510.18573) | [github](https://github.com/CriliasMiller/Kaleido)

---

**CogVideoX: Text-to-Video Diffusion Models with an Expert Transformer**
<small>**Zhuoyi Yang\***, Jiayan Teng\*, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, et al.</small>
*ICLR 2025*
[arXiv](https://arxiv.org/abs/2408.06072) | [github(12k stars)](https://github.com/THUDM/CogVideo)

---

**MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models**
<small>Wenyi Hong\*, Yean Cheng\*, **Zhuoyi Yang\***, Weihan Wang, et al.</small>
*CVPR 2025*
[arXiv](https://arxiv.org/abs/2501.02955) | [github](https://github.com/zai-org/MotionBench)

---

**Inf-DiT: Upsampling Any-Resolution Image with Memory-Efficient Diffusion Transformer**
<small>**Zhuoyi Yang**, Heyang Jiang, Wenyi Hong, Jiayan Teng, Wendi Zheng, Yuxiao Dong, Ming Ding, Jie Tang</small>
*ECCV 2024*
[arXiv](https://arxiv.org/abs/2405.04312) | [github](https://github.com/zai-org/Inf-DiT)

---

**CogVLM: Visual Expert for Pretrained Language Models**
<small>Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, **Zhuoyi Yang**, et al.</small>
*NeurIPS 2024*
[arXiv](https://arxiv.org/abs/2311.03079) | [github(6.7k stars)](https://github.com/zai-org/CogVLM)

---

**CogVLM2: Visual Language Models for Image and Video Understanding**
<small>Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, **Zhuoyi Yang**, et al.</small>
*arXiv preprint arXiv:2408.16500*
[arXiv](https://arxiv.org/abs/2408.16500) | [github](https://github.com/zai-org/CogVLM2)

---

**CogView: Mastering Text-to-Image Generation via Transformers**
<small>Ming Ding, **Zhuoyi Yang**, Wenyi Hong, et al.</small>
*NeurIPS 2021*
[arXiv](https://arxiv.org/abs/2105.13290) | [github(1.8k stars)](https://github.com/zai-org/CogView)

---

**CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion**
<small>Wendi Zheng, Jiayan Teng, **Zhuoyi Yang**, et al.</small>
*ECCV 2024*
[arXiv](https://arxiv.org/abs/2403.05121) | [github(1.1k stars)](https://github.com/zai-org/CogView4)

---

**Parameter-Efficient Tuning Makes a Good Classification Head**
<small>**Zhuoyi Yang**, Ming Ding, Yanhui Guo, Qingsong Lv, Jie Tang</small>
*EMNLP 2022*
[arXiv](https://arxiv.org/abs/2210.16771) | [github](https://github.com/THUDM/Efficient-Head-Finetuning)

---

**GLM-130B: An Open Bilingual Pre-Trained Model**
<small>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, **Zhuoyi Yang**, et al.</small>
*ICLR 2023*
[arXiv](https://arxiv.org/abs/2210.02414) | [github(7.7k stars)](https://github.com/zai-org/GLM-130B)


---

For a complete list, please see my [Google Scholar profile](https://scholar.google.com/citations?user=tgAt-gEAAAAJ).

---

© 2025 Zhuoyi Yang · Built with [Academic Pages](https://github.com/academicpages/academicpages.github.io)